# Data Directory

This directory serves as the workspace for the application's data ingestion pipeline. It contains the input files, intermediate processed files, and the final output data.

**IMPORTANT:** With the exception of sample input files, the contents of this directory are generated by the application at runtime. They are intentionally excluded from the Git repository via the `.gitignore` file and should **not** be committed.

## Directory Structure and Data Flow


data/
├── input/
│ └── pdf <-- STEP 1: Place raw PDFs here
├── processed/
│ ├── txt <-- STEP 2: PDF parser writes .txt files here
│ └── json <-- STEP 3: Text parser writes .json files here
└── output/
└── sql <-- STEP 4: JSON loader writes final .sql files here


### 1. `input/`

*   **Purpose:** Contains the source PDF documents that need to be processed.
*   **Process:** This is the starting point of the pipeline. The `PdfReader` program in the `com.budgetingestion.pdf` package reads files from this directory.

### 2. `processed/`

This directory holds the intermediate files that are generated during the pipeline. These are useful for debugging the process at each step.

*   **`processed/txt/`**
    *   **Purpose:** Stores the raw text extracted from the PDFs.
    *   **Process:** Created by the `PdfReader` program. Read by the `TextToJsonParser` program.

*   **`processed/json/`**
    *   **Purpose:** Stores the structured JSON data created from the raw text.
    *   **Process:** Created by the `TextToJsonParser` program. Read by the `JsonToSqlLoader` program.

### 3. `output/sql/`

*   **Purpose:** Contains the final, consumable output of the pipeline.
*   **Process:** The `.sql` files in this directory are generated by the `JsonToSqlLoader` program. They can then be used by other teams or processes to load the data into a database.
